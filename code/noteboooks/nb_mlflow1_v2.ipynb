{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7 - Prep the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction Cleaning Finale\n",
    "def cleaning(features, test_features, encoding=\"ohe\"):\n",
    "    \"\"\"Fonction cleaning finale\"\"\"\n",
    "    # Extract the ids\n",
    "    train_ids = features[\"SK_ID_CURR\"]\n",
    "    test_ids = test_features[\"SK_ID_CURR\"]\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features[\"TARGET\"]\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns=[\"SK_ID_CURR\", \"TARGET\"])\n",
    "    test_features = test_features.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == \"ohe\":\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join=\"inner\", axis=1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = \"auto\"\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == \"le\":\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == \"object\":\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(\n",
    "                    np.array(features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "                test_features[col] = label_encoder.transform(\n",
    "                    np.array(test_features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print(\"Training Data Shape: \", features.shape)\n",
    "    print(\"Testing Data Shape: \", test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Impute the domainnomial features\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    features = imputer.fit_transform(features)\n",
    "    test_features = imputer.transform(test_features)\n",
    "\n",
    "    # Scale the domainnomial features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    return feature_names, labels, features, test_features, train_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv(\"../../data/raw/application_train.csv\")\n",
    "print(\"Training data shape: \", app_train.shape)\n",
    "app_train.head(2)\n",
    "\n",
    "# Testing data features\n",
    "app_test = pd.read_csv(\"../../data/raw/application_test.csv\")\n",
    "print(\"Testing data shape: \", app_test.shape)\n",
    "app_test.head(2)\n",
    "\n",
    "# copy to add fe\n",
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "# feature engineering with domain knowledge variables\n",
    "app_train_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_CREDIT\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"CREDIT_TERM\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_train_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_train_domain[\"DAYS_EMPLOYED\"] / app_train_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "app_test_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_CREDIT\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"CREDIT_TERM\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_test_domain[\"DAYS_EMPLOYED\"] / app_test_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "# Create an anomalous flag column\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "# Replace the anomalous values with nan\n",
    "app_train_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 246)\n",
      "Testing Data Shape:  (48744, 246)\n"
     ]
    }
   ],
   "source": [
    "liste_features, y_train, X_train, X_test, train_ids, test_ids = cleaning(\n",
    "    app_train_domain, app_test_domain, encoding=\"ohe\"\n",
    ")\n",
    "train_final = pd.DataFrame(X_train, columns=liste_features)\n",
    "train_final[\"LABELS\"] = y_train\n",
    "train_final[\"SK_ID_CURR\"] = train_ids\n",
    "\n",
    "test_final = pd.DataFrame(X_test, columns=liste_features)\n",
    "test_final[\"SK_ID_CURR\"] = test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "scoring = {\n",
    "    \"roc_auc\": make_scorer(roc_auc_score),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n",
      "Resampled class distribution: TARGET\n",
      "1    282686\n",
      "0    282686\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"Original class distribution:\", pd.Series(y_train).value_counts())\n",
    "print(\"Resampled class distribution:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression\",\n",
    "        {\"C\": 10, \"tol\": 0.001, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -1},\n",
    "        LogisticRegression(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest with SMOTE\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train_resampled, y_train_resampled),\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.0s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.6s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models, cv=5, scoring=scoring):\n",
    "    \"\"\"\n",
    "    Evaluate a list of models using cross-validation and store the results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - models: List of tuples, each containing:\n",
    "        - model_name: Name of the model (str)\n",
    "        - params: Dictionary of parameters for the model\n",
    "        - model: The model instance\n",
    "        - train_data: Tuple containing (X_train, y_train)\n",
    "    - cv: Number of cross-validation folds (int)\n",
    "    - scoring: Scoring metric for cross-validation (str)\n",
    "\n",
    "    Returns:\n",
    "    - cv_results_dict: Dictionary containing cross-validation results for each model\n",
    "    \"\"\"\n",
    "    cv_results_dict = {}\n",
    "\n",
    "    for model_name, params, model, train_data in models:\n",
    "        # Since we use SMOTE earlier, the train_data is not always the train set we have as X_train.\n",
    "        # We create X_train_fromtrainset to avoid confusion and error in the code latter when we will use X_train\n",
    "        X_train_fromtrainset = train_data[0]\n",
    "        y_train_fromtrainset = train_data[1]\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            model.set_params(**params),\n",
    "            X_train_fromtrainset,\n",
    "            y_train_fromtrainset,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            return_estimator=True,\n",
    "        )\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        cv_results_dict[model_name] = cv_results\n",
    "\n",
    "    return cv_results_dict\n",
    "\n",
    "\n",
    "cv_results_dict = evaluate_models(models, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/3 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "2025/04/09 15:14:49 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "Registered model 'Logistic Regression_reg' already exists. Creating a new version of this model...\n",
      "2025/04/09 15:16:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Logistic Regression_reg, version 8\n",
      "Created version '8' of model 'Logistic Regression_reg'.\n",
      "Processing models:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:44<03:28, 104.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run Logistic Regression at: http://localhost:5001/#/experiments/201311528320565050/runs/ba14492b2f294889a9268f844556c62f\n",
      "ðŸ§ª View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "2025/04/09 15:16:50 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "Registered model 'Random Forest_reg' already exists. Creating a new version of this model...\n",
      "2025/04/09 15:18:09 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Random Forest_reg, version 6\n",
      "Created version '6' of model 'Random Forest_reg'.\n",
      "Processing models:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [03:51<01:57, 117.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run Random Forest at: http://localhost:5001/#/experiments/201311528320565050/runs/44021e6eed95485ea679ca610b1b4a62\n",
      "ðŸ§ª View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "2025/04/09 15:18:56 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "Registered model 'Random Forest with SMOTE_reg' already exists. Creating a new version of this model...\n",
      "2025/04/09 15:20:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Random Forest with SMOTE_reg, version 6\n",
      "Created version '6' of model 'Random Forest with SMOTE_reg'.\n",
      "Processing models: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [05:52<00:00, 117.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run Random Forest with SMOTE at: http://localhost:5001/#/experiments/201311528320565050/runs/b7d10d7839ad44048ed25fb6f3e79b7d\n",
      "ðŸ§ª View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"MLflow try2\")\n",
    "# might need to run 'mlflow server --host 127.0.0.1 --port 5001' in the terminal if issues\n",
    "\n",
    "for i, element in enumerate(tqdm(models, desc=\"Processing models\")):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "\n",
    "    # The signature wants the input (X_train) and the output (y_train_pred)\n",
    "    # We fit the model so we can use .predict(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    signature = infer_signature(X_train, model.predict(X_train), params=params)\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        if model_name in cv_results_dict:\n",
    "            metrics = cv_results_dict[model_name]\n",
    "            mlflow.log_metrics(  # we take the mean of the metric since we have 5 cv results for each model\n",
    "                {\n",
    "                    \"roc_auc\": metrics.get(\"test_roc_auc\", []).mean(),\n",
    "                    \"accuracy\": metrics.get(\"test_accuracy\", []).mean(),\n",
    "                    \"precision\": metrics.get(\"test_precision\", []).mean(),\n",
    "                    \"recall\": metrics.get(\"test_recall\", []).mean(),\n",
    "                    \"f1\": metrics.get(\"test_f1\", []).mean(),\n",
    "                    \"fit_time\": metrics.get(\"fit_time\", []).mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train,\n",
    "            registered_model_name=f\"{model_name}_reg\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sell credits to customers, each assigned a credit score reflecting their repayment probability. Our model calculates this probability, and we must decide the acceptable risk level for non-repayment.\n",
    "\n",
    "In the credit sector, failed contracts are costlier than successful ones, so we aim to minimize failures. The simplest solutionâ€”not selling any contractsâ€”guarantees no failures but isn't plausible.\n",
    "\n",
    "We must balance risk to sustain activity, setting thresholds to reject high-risk contracts while not rejecting too many good ones. This can be modeled as an optimization problem for activity costs.\n",
    "\n",
    "There are four possible outcomes:\n",
    "- True positives/negatives: Correct predictions.\n",
    "- False positives: Customers repay, but the model predicts failure.\n",
    "- False negatives: Customers fail to repay, but the model predicts success.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Set the ratio between **CFP** (Cost of False Positives) and **CFN** (Cost of False Negatives) based on business knowledge.\n",
    "2. Compute the total cost and select the smallest one.\n",
    "\n",
    "**Total Cost = (FP Ã— CFP) + (FN Ã— CFN)**\n",
    "\n",
    "Since false positives are costlier, **CFP > CFN**. To find the best model, compute the total cost for each and choose the one with the lowest cost. This approach works for any **CFP** to **CFN** ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary ratio: 1 FP to 10 FN\n",
    "# Convert probabilities to class labels\n",
    "y_pred_classes = (y_pred >= 0.091).astype(int)\n",
    "# For testing: the RandomForestClassifier is the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [307511, 48744]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\n",
      "\u001b[1;32m      5\u001b[0m     cm,\n",
      "\u001b[1;32m      6\u001b[0m     annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# yticklabels=unique_categories,\u001b[39;00m\n",
      "\u001b[1;32m     11\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n",
      "\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n",
      "\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n",
      "\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[1;32m    215\u001b[0m     ):\n",
      "\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n",
      "\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n",
      "\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n",
      "\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n",
      "\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n",
      "\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n",
      "\u001b[1;32m    226\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/sklearn/metrics/_classification.py:340\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n",
      "\u001b[1;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n",
      "\u001b[1;32m    258\u001b[0m \n",
      "\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m(np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n",
      "\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    339\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n",
      "\u001b[0;32m--> 340\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/sklearn/metrics/_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n",
      "\u001b[1;32m     72\u001b[0m \n",
      "\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[0;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n",
      "\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n",
      "\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n",
      "\u001b[1;32m    478\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [307511, 48744]"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    # xticklabels=unique_categories,\n",
    "    # yticklabels=unique_categories,\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "# plt.savefig(\"../../figures/cm_bert.png\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
