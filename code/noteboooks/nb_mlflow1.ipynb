{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7 - Prep the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction Cleaning Finale\n",
    "def cleaning(features, test_features, encoding=\"ohe\"):\n",
    "    \"\"\"Fonction cleaning finale\"\"\"\n",
    "    # Extract the ids\n",
    "    train_ids = features[\"SK_ID_CURR\"]\n",
    "    test_ids = test_features[\"SK_ID_CURR\"]\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features[\"TARGET\"]\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns=[\"SK_ID_CURR\", \"TARGET\"])\n",
    "    test_features = test_features.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == \"ohe\":\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join=\"inner\", axis=1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = \"auto\"\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == \"le\":\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == \"object\":\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(\n",
    "                    np.array(features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "                test_features[col] = label_encoder.transform(\n",
    "                    np.array(test_features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print(\"Training Data Shape: \", features.shape)\n",
    "    print(\"Testing Data Shape: \", test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Impute the domainnomial features\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    features = imputer.fit_transform(features)\n",
    "    test_features = imputer.transform(test_features)\n",
    "\n",
    "    # Scale the domainnomial features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    return feature_names, labels, features, test_features, train_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv(\"../../data/raw/application_train.csv\")\n",
    "print(\"Training data shape: \", app_train.shape)\n",
    "app_train.head(2)\n",
    "\n",
    "# Testing data features\n",
    "app_test = pd.read_csv(\"../../data/raw/application_test.csv\")\n",
    "print(\"Testing data shape: \", app_test.shape)\n",
    "app_test.head(2)\n",
    "\n",
    "# copy to add fe\n",
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "# feature engineering with domain knowledge variables\n",
    "app_train_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_CREDIT\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"CREDIT_TERM\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_train_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_train_domain[\"DAYS_EMPLOYED\"] / app_train_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "app_test_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_CREDIT\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"CREDIT_TERM\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_test_domain[\"DAYS_EMPLOYED\"] / app_test_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "# Create an anomalous flag column\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "# Replace the anomalous values with nan\n",
    "app_train_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 246)\n",
      "Testing Data Shape:  (48744, 246)\n"
     ]
    }
   ],
   "source": [
    "liste_features, y_train, X_train, X_test, train_ids, test_ids = cleaning(\n",
    "    app_train_domain, app_test_domain, encoding=\"ohe\"\n",
    ")\n",
    "train_final = pd.DataFrame(X_train, columns=liste_features)\n",
    "train_final[\"LABELS\"] = y_train\n",
    "train_final[\"SK_ID_CURR\"] = train_ids\n",
    "\n",
    "test_final = pd.DataFrame(X_test, columns=liste_features)\n",
    "test_final[\"SK_ID_CURR\"] = test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "scoring = {\n",
    "    \"roc_auc\": make_scorer(roc_auc_score),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n",
      "Resampled class distribution: TARGET\n",
      "1    282686\n",
      "0    282686\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"Original class distribution:\", pd.Series(y_train).value_counts())\n",
    "print(\"Resampled class distribution:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[125], line 8\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Models\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[1;32m      3\u001b[0m     (\n",
      "\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m},\n",
      "\u001b[1;32m      6\u001b[0m         LogisticRegression(),\n",
      "\u001b[1;32m      7\u001b[0m         (X_train, y_train),\n",
      "\u001b[0;32m----> 8\u001b[0m         (X_train, \u001b[43my_pred\u001b[49m)\n",
      "\u001b[1;32m      9\u001b[0m     ),\n",
      "\u001b[1;32m     10\u001b[0m     (\n",
      "\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m     12\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m},\n",
      "\u001b[1;32m     13\u001b[0m         RandomForestClassifier(),\n",
      "\u001b[1;32m     14\u001b[0m         (X_train, y_train),\n",
      "\u001b[1;32m     15\u001b[0m     ),\n",
      "\u001b[1;32m     16\u001b[0m     (\n",
      "\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest with SMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m     18\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m},\n",
      "\u001b[1;32m     19\u001b[0m         RandomForestClassifier(),\n",
      "\u001b[1;32m     20\u001b[0m         (X_train_resampled, y_train_resampled),\n",
      "\u001b[1;32m     21\u001b[0m     ),\n",
      "\u001b[1;32m     22\u001b[0m ]\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression\",\n",
    "        {\"C\": 10, \"tol\": 0.001, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -1},\n",
    "        LogisticRegression(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest with SMOTE\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train_resampled, y_train_resampled),\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   11.9s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   28.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.4s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models, cv=5, scoring=scoring):\n",
    "    \"\"\"\n",
    "    Evaluate a list of models using cross-validation and store the results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - models: List of tuples, each containing:\n",
    "        - model_name: Name of the model (str)\n",
    "        - params: Dictionary of parameters for the model\n",
    "        - model: The model instance\n",
    "        - train_data: Tuple containing (X_train, y_train)\n",
    "    - cv: Number of cross-validation folds (int)\n",
    "    - scoring: Scoring metric for cross-validation (str)\n",
    "\n",
    "    Returns:\n",
    "    - cv_results_dict: Dictionary containing cross-validation results for each model\n",
    "    \"\"\"\n",
    "    cv_results_dict = {}\n",
    "\n",
    "    for model_name, params, model, train_data in models:\n",
    "        X_train = train_data[0]\n",
    "        y_train = train_data[1]\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            model.set_params(**params),\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            return_estimator=True,\n",
    "        )\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        cv_results_dict[model_name] = cv_results\n",
    "\n",
    "    return cv_results_dict\n",
    "\n",
    "\n",
    "cv_results_dict = evaluate_models(models, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"MLflow try2\")\n",
    "# might need to run 'mlflow server --host 127.0.0.1 --port 5001' in the terminal if issues\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "        for model_name in cv_results_dict:\n",
    "            mlflow.log_metrics(  # we take mean of the metric since we have 5 cv results for each model\n",
    "                {\n",
    "                    \"roc_auc\": cv_results_dict[model_name][\"test_roc_auc\"].mean(),\n",
    "                    \"accuracy\": cv_results_dict[model_name][\"test_accuracy\"].mean(),\n",
    "                    \"precision\": cv_results_dict[model_name][\"test_precision\"].mean(),\n",
    "                    \"recall\": cv_results_dict[model_name][\"test_recall\"].mean(),\n",
    "                    \"f1\": cv_results_dict[model_name][\"test_f1\"].mean(),\n",
    "                    \"fit_time\": cv_results_dict[model_name][\"fit_time\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "for model_info in models:\n",
    "    model = model_info[2]\n",
    "    data = model_info[3][0]\n",
    "\n",
    "    model_info[2].fit(X_train, y_train)\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Infer the signature\n",
    "    signature = infer_signature(data, predictions)\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\", input_example=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/08 12:21:30 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: ValueError(\"could not convert string to float: 'auto'\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n",
      "2025/04/08 12:21:33 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/04/08 12:21:34 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": \"auto\"\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: Expected 2D array, got scalar array instead:\n",
      "array=auto.\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
      "2025/04/08 12:21:34 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: ValueError(\"could not convert string to float: 'auto'\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Logistic Regression at: http://localhost:5001/#/experiments/201311528320565050/runs/83001a8c0d6547c7a3d1af66e47516b0\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/08 12:21:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/04/08 12:21:38 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": \"auto\"\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: could not convert string to float: np.str_('auto')\n",
      "2025/04/08 12:21:39 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: ValueError(\"could not convert string to float: 'auto'\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Random Forest at: http://localhost:5001/#/experiments/201311528320565050/runs/f89cfb8c9391422f955392e0a55e674a\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/08 12:21:43 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "2025/04/08 12:21:43 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": \"auto\"\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: could not convert string to float: np.str_('auto')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Random Forest with SMOTE at: http://localhost:5001/#/experiments/201311528320565050/runs/914ff1eae3ba48d8af3ae774c4bfd69d\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"MLflow try2\")\n",
    "# might need to run 'mlflow server --host 127.0.0.1 --port 5001' in the terminal if issues\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        if model_name in cv_results_dict:\n",
    "            metrics = cv_results_dict[model_name]\n",
    "            mlflow.log_metrics(  # we take mean of the metric since we have 5 cv results for each model\n",
    "                {\n",
    "                    \"roc_auc\": metrics.get(\"test_roc_auc\", []).mean(),\n",
    "                    \"accuracy\": metrics.get(\"test_accuracy\", []).mean(),\n",
    "                    \"precision\": metrics.get(\"test_precision\", []).mean(),\n",
    "                    \"recall\": metrics.get(\"test_recall\", []).mean(),\n",
    "                    \"f1\": metrics.get(\"test_f1\", []).mean(),\n",
    "                    \"fit_time\": metrics.get(\"fit_time\", []).mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00000000e+00, 1.51186991e-03, 9.02865169e-02, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 2.08891949e-03, 3.11735955e-01, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 3.57770742e-04, 2.24719101e-02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 1.08870022e-03, 1.57968539e-01, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.24258010e-03, 8.11752809e-02, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.12717019e-03, 1.57303371e-01, ...,\n",
       "         0.00000000e+00, 1.00000000e+00, 0.00000000e+00]],\n",
       "       shape=(307511, 246)),\n",
       " 0         1\n",
       " 1         0\n",
       " 2         0\n",
       " 3         0\n",
       " 4         0\n",
       "          ..\n",
       " 307506    0\n",
       " 307507    0\n",
       " 307508    0\n",
       " 307509    1\n",
       " 307510    0\n",
       " Name: TARGET, Length: 307511, dtype: int64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Random Forest\n",
      "Random Forest with SMOTE\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(models):\n",
    "    print(f\"{element[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9537513860528317)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_dict[\"Random Forest with SMOTE\"][\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.51186991e-03, 9.02865169e-02, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.08891949e-03, 3.11735955e-01, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 3.57770742e-04, 2.24719101e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 1.08870022e-03, 1.57968539e-01, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.24258010e-03, 8.11752809e-02, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.12717019e-03, 1.57303371e-01, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00]],\n",
       "      shape=(307511, 246))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0][3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run loud-mule-333 at: http://localhost:5001/#/experiments/201311528320565050/runs/f3196f2510ae44289112d4b6a9e97887\n",
      "🧪 View experiment at: http://localhost:5001/#/experiments/201311528320565050\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "2025/04/07 16:04:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "Registered model 'tracking-quickstart' already exists. Creating a new version of this model...\n",
      "2025/04/07 16:05:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: tracking-quickstart, version 5\n",
      "Created version '5' of model 'tracking-quickstart'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run clumsy-koi-940 at: http://127.0.0.1:5000/#/experiments/726532937351277124/runs/2e70af58225c446989fa861ada3fd324\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/726532937351277124\n"
     ]
    }
   ],
   "source": [
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow try1\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log each metric individually\n",
    "    mlflow.log_metric(\"accuracy\", cv_results[\"test_accuracy\"].mean())\n",
    "    mlflow.log_metric(\"precision\", cv_results[\"test_precision\"].mean())\n",
    "    mlflow.log_metric(\"recall\", cv_results[\"test_recall\"].mean())\n",
    "    mlflow.log_metric(\"f1\", cv_results[\"test_f1\"].mean())\n",
    "\n",
    "    # Log the loss metric\n",
    "    # mlflow.log_metric(\n",
    "    #     {\n",
    "    #         #'roc_auc_score': cv_results['test_roc_auc'],\n",
    "    #         'accuracy': (cv_results['test_accuracy'].mean()),\n",
    "    #         'precision': (cv_results['test_precision'].mean()),\n",
    "    #         'recall': (cv_results['test_recall'].mean()),\n",
    "    #         'f1': (cv_results['test_f1'].mean()),\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"RandomForest for credit classification\")\n",
    "\n",
    "    random_forest_domain.fit(X_train, y_train)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, random_forest_domain.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=random_forest_domain,\n",
    "        artifact_path=\"credit_model_test1\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
