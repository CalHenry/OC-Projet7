{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7 - Prep the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction Cleaning Finale\n",
    "def cleaning(features, test_features, encoding=\"ohe\"):\n",
    "    \"\"\"Fonction cleaning finale\"\"\"\n",
    "    # Extract the ids\n",
    "    train_ids = features[\"SK_ID_CURR\"]\n",
    "    test_ids = test_features[\"SK_ID_CURR\"]\n",
    "\n",
    "    # Extract the labels for training\n",
    "    labels = features[\"TARGET\"]\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns=[\"SK_ID_CURR\", \"TARGET\"])\n",
    "    test_features = test_features.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    if encoding == \"ohe\":\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join=\"inner\", axis=1)\n",
    "\n",
    "        # No categorical indices to record\n",
    "        cat_indices = \"auto\"\n",
    "\n",
    "    # Integer label encoding\n",
    "    elif encoding == \"le\":\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == \"object\":\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(\n",
    "                    np.array(features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "                test_features[col] = label_encoder.transform(\n",
    "                    np.array(test_features[col].astype(str)).reshape((-1,))\n",
    "                )\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "\n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "\n",
    "    print(\"Training Data Shape: \", features.shape)\n",
    "    print(\"Testing Data Shape: \", test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Impute the domainnomial features\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    features = imputer.fit_transform(features)\n",
    "    test_features = imputer.transform(test_features)\n",
    "\n",
    "    # Scale the domainnomial features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    features = scaler.fit_transform(features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    return feature_names, labels, features, test_features, train_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "app_train = pd.read_csv(\"../../data/raw/application_train.csv\")\n",
    "print(\"Training data shape: \", app_train.shape)\n",
    "app_train.head(2)\n",
    "\n",
    "# Testing data features\n",
    "app_test = pd.read_csv(\"../../data/raw/application_test.csv\")\n",
    "print(\"Testing data shape: \", app_test.shape)\n",
    "app_test.head(2)\n",
    "\n",
    "# copy to add fe\n",
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "# feature engineering with domain knowledge variables\n",
    "app_train_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_CREDIT\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_train_domain[\"CREDIT_TERM\"] = (\n",
    "    app_train_domain[\"AMT_ANNUITY\"] / app_train_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_train_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_train_domain[\"DAYS_EMPLOYED\"] / app_train_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "app_test_domain[\"CREDIT_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_CREDIT\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"ANNUITY_INCOME_PERCENT\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_INCOME_TOTAL\"]\n",
    ")\n",
    "app_test_domain[\"CREDIT_TERM\"] = (\n",
    "    app_test_domain[\"AMT_ANNUITY\"] / app_test_domain[\"AMT_CREDIT\"]\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_PERCENT\"] = (\n",
    "    app_test_domain[\"DAYS_EMPLOYED\"] / app_test_domain[\"DAYS_BIRTH\"]\n",
    ")\n",
    "\n",
    "# Create an anomalous flag column\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "# Replace the anomalous values with nan\n",
    "app_train_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED\"] == 365243\n",
    "app_test_domain[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "app_train_domain[\"DAYS_EMPLOYED_ANOM\"] = app_train_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")\n",
    "app_test_domain[\"DAYS_EMPLOYED_ANOM\"] = app_test_domain[\"DAYS_EMPLOYED_ANOM\"].astype(\n",
    "    \"int32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 246)\n",
      "Testing Data Shape:  (48744, 246)\n"
     ]
    }
   ],
   "source": [
    "liste_features, y_train, X_train, X_test, train_ids, test_ids = cleaning(\n",
    "    app_train_domain, app_test_domain, encoding=\"ohe\"\n",
    ")\n",
    "train_final = pd.DataFrame(X_train, columns=liste_features)\n",
    "train_final[\"LABELS\"] = y_train\n",
    "train_final[\"SK_ID_CURR\"] = train_ids\n",
    "\n",
    "test_final = pd.DataFrame(X_test, columns=liste_features)\n",
    "test_final[\"SK_ID_CURR\"] = test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "scoring = {\n",
    "    \"roc_auc\": make_scorer(roc_auc_score),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: TARGET\n",
      "0    282686\n",
      "1     24825\n",
      "Name: count, dtype: int64\n",
      "Resampled class distribution: TARGET\n",
      "1    282686\n",
      "0    282686\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"Original class distribution:\", pd.Series(y_train).value_counts())\n",
    "print(\"Resampled class distribution:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = [\n",
    "    (\n",
    "        \"Logistic Regression\",\n",
    "        {\"C\": 10, \"tol\": 0.001, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -1},\n",
    "        LogisticRegression(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train, y_train),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest with SMOTE\",\n",
    "        {\"n_estimators\": 100, \"random_state\": 42, \"verbose\": 1, \"n_jobs\": -4},\n",
    "        RandomForestClassifier(),\n",
    "        (X_train_resampled, y_train_resampled),\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   11.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   11.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   11.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   12.5s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.8s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   27.3s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models, cv=5, scoring=scoring):\n",
    "    \"\"\"\n",
    "    Evaluate a list of models using cross-validation and store the results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - models: List of tuples, each containing:\n",
    "        - model_name: Name of the model (str)\n",
    "        - params: Dictionary of parameters for the model\n",
    "        - model: The model instance\n",
    "        - train_data: Tuple containing (X_train, y_train)\n",
    "    - cv: Number of cross-validation folds (int)\n",
    "    - scoring: Scoring metric for cross-validation (str)\n",
    "\n",
    "    Returns:\n",
    "    - cv_results_dict: Dictionary containing cross-validation results for each model\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store results\n",
    "    cv_results_dict = {}\n",
    "\n",
    "    for model_name, params, model, train_data in models:\n",
    "        X_train = train_data[0]\n",
    "        y_train = train_data[1]\n",
    "\n",
    "        cv_results = cross_validate(\n",
    "            model.set_params(**params),\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            return_estimator=True,\n",
    "        )\n",
    "\n",
    "        # Store the results in the dictionary\n",
    "        cv_results_dict[model_name] = cv_results\n",
    "\n",
    "    return cv_results_dict\n",
    "\n",
    "\n",
    "cv_results_dict = evaluate_models(models, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize MLflow\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMLflow try1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n",
      "\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n",
      "\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 157\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n",
      "\u001b[1;32m    159\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/client.py:1698\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n",
      "\u001b[1;32m   1667\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n",
      "\u001b[1;32m   1668\u001b[0m \n",
      "\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n",
      "\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/client.py:586\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n",
      "\u001b[1;32m    579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n",
      "\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:641\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n",
      "\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    640\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n",
      "\u001b[0;32m--> 641\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n",
      "\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n",
      "\u001b[1;32m     88\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n",
      "\u001b[1;32m     89\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n",
      "\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:392\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n",
      "\u001b[1;32m    389\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "\u001b[1;32m    390\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n",
      "\u001b[0;32m--> 392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    393\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;32m    394\u001b[0m js_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_to_parse)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:255\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n",
      "\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    251\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    254\u001b[0m         )\n",
      "\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n",
      "\u001b[1;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Response body: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    257\u001b[0m             error_code\u001b[38;5;241m=\u001b[39mget_error_code(response\u001b[38;5;241m.\u001b[39mstatus_code),\n",
      "\u001b[1;32m    258\u001b[0m         )\n",
      "\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\u001b[39;00m\n",
      "\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# response\u001b[39;00m\n",
      "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endpoint\u001b[38;5;241m.\u001b[39mstartswith(_REST_API_PATH_PREFIX) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n",
      "\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"MLflow try1\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize MLflow\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMLflow try1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:5001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/fluent.py:157\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n",
      "\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n",
      "\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 157\u001b[0m         experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n",
      "\u001b[1;32m    159\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/client.py:1698\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Experiment]:\n",
      "\u001b[1;32m   1667\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n",
      "\u001b[1;32m   1668\u001b[0m \n",
      "\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m        Lifecycle_stage: active\u001b[39;00m\n",
      "\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/client.py:586\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n",
      "\u001b[1;32m    579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        name: The experiment name.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n",
      "\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:641\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n",
      "\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    640\u001b[0m     req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n",
      "\u001b[0;32m--> 641\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n",
      "\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n",
      "\u001b[1;32m     88\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m _METHOD_TO_INFO[api]\n",
      "\u001b[1;32m     89\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n",
      "\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:392\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n",
      "\u001b[1;32m    389\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n",
      "\u001b[1;32m    390\u001b[0m     response \u001b[38;5;241m=\u001b[39m http_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs)\n",
      "\u001b[0;32m--> 392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    393\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;32m    394\u001b[0m js_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_to_parse)\n",
      "\n",
      "File \u001b[0;32m~/Desktop/OC/Projet7/.pixi/envs/default/lib/python3.13/site-packages/mlflow/utils/rest_utils.py:255\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n",
      "\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    251\u001b[0m         base_msg \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != 200\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    254\u001b[0m         )\n",
      "\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n",
      "\u001b[1;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Response body: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m    257\u001b[0m             error_code\u001b[38;5;241m=\u001b[39mget_error_code(response\u001b[38;5;241m.\u001b[39mstatus_code),\n",
      "\u001b[1;32m    258\u001b[0m         )\n",
      "\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\u001b[39;00m\n",
      "\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# response\u001b[39;00m\n",
      "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endpoint\u001b[38;5;241m.\u001b[39mstartswith(_REST_API_PATH_PREFIX) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_parse_as_json_object(response\u001b[38;5;241m.\u001b[39mtext):\n",
      "\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"MLflow try1\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    params = element[1]\n",
    "    model = element[2]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_params(params)\n",
    "        for model_name in cv_results_dict:\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"roc_auc\": cv_results_dict[model_name][\"test_roc_auc\"],\n",
    "                    \"accuracy\": cv_results_dict[model_name][\"accuracy\"],\n",
    "                    \"precision\": cv_results_dict[model_name][\"precision\"],\n",
    "                    \"recall\": cv_results_dict[model_name][\"test_recall\"],\n",
    "                    \"f1\": cv_results_dict[model_name][\"f1\"],\n",
    "                    \"fit_time\": cv_results_dict[model_name][\"fit_time\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-4)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-4)]: Done  36 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-4)]: Done 100 out of 100 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "2025/04/07 16:04:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=7)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "Registered model 'tracking-quickstart' already exists. Creating a new version of this model...\n",
      "2025/04/07 16:05:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: tracking-quickstart, version 5\n",
      "Created version '5' of model 'tracking-quickstart'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run clumsy-koi-940 at: http://127.0.0.1:5000/#/experiments/726532937351277124/runs/2e70af58225c446989fa861ada3fd324\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/726532937351277124\n"
     ]
    }
   ],
   "source": [
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow try1\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log each metric individually\n",
    "    mlflow.log_metric(\"accuracy\", cv_results[\"test_accuracy\"].mean())\n",
    "    mlflow.log_metric(\"precision\", cv_results[\"test_precision\"].mean())\n",
    "    mlflow.log_metric(\"recall\", cv_results[\"test_recall\"].mean())\n",
    "    mlflow.log_metric(\"f1\", cv_results[\"test_f1\"].mean())\n",
    "\n",
    "    # Log the loss metric\n",
    "    # mlflow.log_metric(\n",
    "    #     {\n",
    "    #         #'roc_auc_score': cv_results['test_roc_auc'],\n",
    "    #         'accuracy': (cv_results['test_accuracy'].mean()),\n",
    "    #         'precision': (cv_results['test_precision'].mean()),\n",
    "    #         'recall': (cv_results['test_recall'].mean()),\n",
    "    #         'f1': (cv_results['test_f1'].mean()),\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"RandomForest for credit classification\")\n",
    "\n",
    "    random_forest_domain.fit(X_train, y_train)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, random_forest_domain.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=random_forest_domain,\n",
    "        artifact_path=\"credit_model_test1\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
