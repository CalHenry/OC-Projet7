{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "%load_ext rich\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Drift Analysis using Evidently for Credit Classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Evidently modules\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataDriftPreset,\n",
    "    DataQualityPreset,\n",
    "    TargetDriftPreset,\n",
    ")\n",
    "from evidently.metrics import *\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.ui.dashboards import CounterAgg, DashboardPanelInfo\n",
    "from evidently.ui.workspace import Workspace, WorkspaceBase\n",
    "\n",
    "# Step 1: Load your datasets\n",
    "# Replace these with your actual data loading code\n",
    "# For example:\n",
    "# X_train = pd.read_csv('X_train.csv')\n",
    "# y_train = pd.read_csv('y_train.csv')\n",
    "# X_val = pd.read_csv('X_val.csv')\n",
    "# y_val = pd.read_csv('y_val.csv')\n",
    "# X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "\n",
    "# For demonstration, I'll create sample data:\n",
    "def create_sample_data(n_samples, with_drift=False):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create features\n",
    "    data = {\n",
    "        \"income\": np.random.normal(50000, 15000, n_samples)\n",
    "        * (1.2 if with_drift else 1),\n",
    "        \"debt_ratio\": np.random.uniform(0.1, 0.8, n_samples)\n",
    "        * (0.9 if with_drift else 1),\n",
    "        \"credit_score\": np.random.normal(700, 100, n_samples)\n",
    "        * (1.1 if with_drift else 1),\n",
    "        \"employment_length\": np.random.poisson(5, n_samples)\n",
    "        * (1.3 if with_drift else 1),\n",
    "        \"age\": np.random.normal(40, 10, n_samples) * (1.1 if with_drift else 1),\n",
    "        \"categorical_education\": np.random.choice(\n",
    "            [\"High School\", \"Bachelor\", \"Master\", \"PhD\"], n_samples\n",
    "        ),\n",
    "        \"categorical_housing\": np.random.choice([\"Own\", \"Rent\", \"Mortgage\"], n_samples),\n",
    "    }\n",
    "\n",
    "    # Add some null values to test data if drift is True\n",
    "    if with_drift:\n",
    "        for col in [\"income\", \"debt_ratio\", \"credit_score\"]:\n",
    "            null_indices = np.random.choice(\n",
    "                n_samples, size=int(n_samples * 0.05), replace=False\n",
    "            )\n",
    "            data[col] = pd.Series(data[col])\n",
    "            data[col].iloc[null_indices] = np.nan\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create target variable (predicting loan repayment)\n",
    "    # Formula: higher income and credit score, lower debt ratio = better repayment\n",
    "    repayment_score = (\n",
    "        data[\"income\"] / 50000 * 0.4\n",
    "        + data[\"credit_score\"] / 700 * 0.4\n",
    "        - data[\"debt_ratio\"] * 0.2\n",
    "    )\n",
    "    threshold = 0.7\n",
    "    # 0 = repays loan, 1 = won't repay loan\n",
    "    target = (repayment_score < threshold).astype(int)\n",
    "\n",
    "    return df, target\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "X_train, y_train = create_sample_data(1000)\n",
    "X_val, y_val = create_sample_data(300)\n",
    "X_test, _ = create_sample_data(200, with_drift=True)  # Test data with drift, no labels\n",
    "\n",
    "# Convert target to DataFrame\n",
    "y_train = pd.DataFrame(y_train, columns=[\"target\"])\n",
    "y_val = pd.DataFrame(y_val, columns=[\"target\"])\n",
    "\n",
    "# Combine features and target for reference and current datasets\n",
    "reference_data = X_val.copy()\n",
    "reference_data[\"target\"] = y_val[\"target\"].values\n",
    "\n",
    "current_data = X_test.copy()\n",
    "# Note: current_data has no target because it's unlabeled\n",
    "\n",
    "# Step 2: Train a simple model to get predictions\n",
    "# This step is just to show how to handle model predictions in the analysis\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train[\"target\"])\n",
    "\n",
    "# Generate predictions for both validation and test sets\n",
    "val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Add predictions to dataframes\n",
    "reference_data[\"prediction\"] = val_pred_proba\n",
    "current_data[\"prediction\"] = test_pred_proba\n",
    "\n",
    "# Step 3: Define column mapping for Evidently\n",
    "# This tells Evidently which columns are features, targets, and predictions\n",
    "column_mapping = ColumnMapping(\n",
    "    target=\"target\",\n",
    "    prediction=\"prediction\",\n",
    "    numerical_features=[\n",
    "        \"income\",\n",
    "        \"debt_ratio\",\n",
    "        \"credit_score\",\n",
    "        \"employment_length\",\n",
    "        \"age\",\n",
    "    ],\n",
    "    categorical_features=[\"categorical_education\", \"categorical_housing\"],\n",
    ")\n",
    "\n",
    "# Step 4: Create Data Drift Report\n",
    "data_drift_report = Report(\n",
    "    metrics=[\n",
    "        DataDriftPreset(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 5: Run the report\n",
    "data_drift_report.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=column_mapping,\n",
    ")\n",
    "\n",
    "# Save the report\n",
    "data_drift_report.save_html(\"data_drift_report.html\")\n",
    "\n",
    "# Step 6: Create a more detailed test suite\n",
    "data_drift_tests = TestSuite(\n",
    "    tests=[\n",
    "        DataDriftTestPreset(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_drift_tests.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=column_mapping,\n",
    ")\n",
    "data_drift_tests.save_html(\"data_drift_tests.html\")\n",
    "\n",
    "# Step 7: Feature-by-feature analysis\n",
    "print(\"Feature-by-feature drift analysis:\")\n",
    "for feature in column_mapping.numerical_features + column_mapping.categorical_features:\n",
    "    # Create a single-feature report\n",
    "    feature_report = Report(metrics=[ColumnDriftMetric(column_name=feature)])\n",
    "\n",
    "    feature_report.run(\n",
    "        reference_data=reference_data,\n",
    "        current_data=current_data,\n",
    "        column_mapping=column_mapping,\n",
    "    )\n",
    "    result = feature_report.as_dict()\n",
    "\n",
    "    # Extract and print drift information\n",
    "    drift_detected = result[\"metrics\"][0][\"result\"][\"drift_detected\"]\n",
    "    drift_score = result[\"metrics\"][0][\"result\"].get(\"drift_score\", \"N/A\")\n",
    "\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"  Drift detected: {drift_detected}\")\n",
    "    print(f\"  Drift score: {drift_score}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Step 8: Analyze prediction drift\n",
    "# Even without labels, we can check if the model's output distribution has changed\n",
    "prediction_drift_report = Report(metrics=[ColumnDriftMetric(column_name=\"prediction\")])\n",
    "\n",
    "prediction_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
    "pred_result = prediction_drift_report.as_dict()\n",
    "\n",
    "print(\"Prediction Distribution Drift:\")\n",
    "print(f\"  Drift detected: {pred_result['metrics'][0]['result']['drift_detected']}\")\n",
    "print(f\"  Drift score: {pred_result['metrics'][0]['result'].get('drift_score', 'N/A')}\")\n",
    "\n",
    "# Step 9: Visualize the distributions of key features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(column_mapping.numerical_features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.kdeplot(reference_data[feature], label=\"Reference (Validation)\", alpha=0.7)\n",
    "    sns.kdeplot(current_data[feature], label=\"Current (Test)\", alpha=0.7)\n",
    "    plt.title(f\"{feature} Distribution\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_distributions.png\")\n",
    "\n",
    "# Step 10: Create a basic monitoring dashboard\n",
    "workspace = Workspace(\"./workspace\")\n",
    "dashboard = workspace.create_dashboard(\"Credit Risk Data Drift\")\n",
    "\n",
    "dashboard.add_panel(\n",
    "    DashboardPanelInfo(\n",
    "        title=\"Data Drift Score\", metric=DataDriftTable(), agg=CounterAgg.last()\n",
    "    )\n",
    ")\n",
    "\n",
    "dashboard.add_panel(\n",
    "    DashboardPanelInfo(\n",
    "        title=\"Prediction Drift\",\n",
    "        metric=ColumnDriftMetric(column_name=\"prediction\"),\n",
    "        agg=CounterAgg.last(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add several features to monitor specifically\n",
    "for feature in [\"income\", \"credit_score\", \"debt_ratio\"]:\n",
    "    dashboard.add_panel(\n",
    "        DashboardPanelInfo(\n",
    "            title=f\"{feature} Drift\",\n",
    "            metric=ColumnDriftMetric(column_name=feature),\n",
    "            agg=CounterAgg.last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "dashboard.save()\n",
    "print(\"Dashboard created at: ./workspace\")\n",
    "\n",
    "# Step 11: Analysis of potential causes of data drift\n",
    "print(\"\\nData Drift Root Cause Analysis:\")\n",
    "\n",
    "# Check for range shifts in numerical features\n",
    "print(\"\\nNumerical Feature Range Analysis:\")\n",
    "for feature in column_mapping.numerical_features:\n",
    "    ref_min, ref_max = reference_data[feature].min(), reference_data[feature].max()\n",
    "    curr_min, curr_max = current_data[feature].min(), current_data[feature].max()\n",
    "\n",
    "    min_change_pct = (\n",
    "        (curr_min - ref_min) / ref_min * 100 if ref_min != 0 else float(\"inf\")\n",
    "    )\n",
    "    max_change_pct = (\n",
    "        (curr_max - ref_max) / ref_max * 100 if ref_max != 0 else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Reference range: [{ref_min:.2f}, {ref_max:.2f}]\")\n",
    "    print(f\"  Current range: [{curr_min:.2f}, {curr_max:.2f}]\")\n",
    "    print(f\"  Min change: {min_change_pct:.2f}%, Max change: {max_change_pct:.2f}%\")\n",
    "\n",
    "# Check for distribution changes in categorical features\n",
    "print(\"\\nCategorical Feature Distribution Analysis:\")\n",
    "for feature in column_mapping.categorical_features:\n",
    "    ref_dist = reference_data[feature].value_counts(normalize=True)\n",
    "    curr_dist = current_data[feature].value_counts(normalize=True)\n",
    "\n",
    "    print(f\"{feature} distribution changes:\")\n",
    "    for category in set(ref_dist.index) | set(curr_dist.index):\n",
    "        ref_val = ref_dist.get(category, 0) * 100\n",
    "        curr_val = curr_dist.get(category, 0) * 100\n",
    "        change = curr_val - ref_val\n",
    "\n",
    "        print(f\"  {category}: {ref_val:.2f}% → {curr_val:.2f}% (Change: {change:.2f}%)\")\n",
    "\n",
    "# Step 12: Summarize findings and recommendations\n",
    "print(\"\\n=== DATA DRIFT ANALYSIS SUMMARY ===\")\n",
    "print(\"Based on the Evidently analysis, the following actions are recommended:\")\n",
    "print(\"1. Review features with significant drift for data quality issues\")\n",
    "print(\"2. Consider retraining the model if prediction distribution has shifted\")\n",
    "print(\"3. Set up continuous monitoring with drift detection thresholds\")\n",
    "print(\n",
    "    \"4. Investigate potential concept drift if available (when labels become available)\"\n",
    ")\n",
    "print(\"5. For features with most drift, consider feature engineering or transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataDriftPreset,\n",
    "    DataQualityPreset,\n",
    "    TargetDriftPreset,\n",
    ")\n",
    "\n",
    "#from evidently.presets import DataSummaryPreset\n",
    "#from evidently.presets import DataSummaryPreset\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "%load_ext rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "app_train_domain = joblib.load(\"../../data/processed/app_train_domain.joblib\")\n",
    "logistic_y_train = joblib.load(\"../../data/processed/logistic_y_train.joblib\")\n",
    "logistic_y_test = joblib.load(\"../../data/processed/logistic_y_test.joblib\")\n",
    "logistic_y_pred = joblib.load(\"../../data/processed/logistic_y_pred.joblib\")\n",
    "app_test_domain = joblib.load(\"../../data/processed/app_test_domain.joblib\")\n",
    "#logistic_y_pred = pd.Series(logistic_y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_y_pred = pd.DataFrame(logistic_y_pred[:, 1], columns=['y_pred'])\n",
    "logistic_y_test = pd.DataFrame(logistic_y_test)\n",
    "logistic_y_train = pd.DataFrame(logistic_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = app_train_domain.select_dtypes(include=['object']).columns\n",
    "numerical_var = app_train_domain.columns.difference(categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    target='y_test',\n",
    "    prediction='y_pred',\n",
    "    numerical_features=numerical_var.to_list(),\n",
    "    categorical_features=categorical_var.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Create Data Drift Report\n",
    "data_drift_report = Report(\n",
    "    metrics=[\n",
    "        DataDriftPreset(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Run the report\n",
    "data_drift_report.run(\n",
    "    reference_data=app_train_domain,\n",
    "    current_data=app_test_domain,\n",
    "    column_mapping=column_mapping,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = app_train_domain.sample(n=100000, random_state=1)\n",
    "sample_test = app_test_domain.sample(n=10000, random_state=1)\n",
    "\n",
    "sample_train = sample_train.drop('TARGET', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Data Drift Report\n",
    "data_drift_report = Report(\n",
    "    metrics=[\n",
    "        DataDriftPreset(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Run the report\n",
    "data_drift_report.run(\n",
    "    reference_data=sample_train,\n",
    "    current_data=sample_test,\n",
    "        column_mapping=column_mapping,\n",
    ")\n",
    "\n",
    "#data_drift_report.save_html(\"data_drift_report.html\")\n",
    "data_drift_report.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
